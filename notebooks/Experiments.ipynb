{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from src.agent import AgentA2C\n",
    "from src.utils import TrajStats, set_seeds\n",
    "from src.envs_wrappers import SubprocEnvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(logits_1, logits_2):\n",
    "    \"\"\"\n",
    "    Computes KL divergence between discrete distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    probs_1 = F.softmax(logits_1, dim=-1)\n",
    "    kl_components = probs_1 * (F.log_softmax(logits_1, dim=-1) - F.log_softmax(logits_2, dim=-1))\n",
    "    return torch.mean(torch.sum(kl_components, dim=0))\n",
    "\n",
    "\n",
    "def get_flat_params(model):\n",
    "    return torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "\n",
    "def set_flat_params(model, flat_params):\n",
    "    ind_start = 0\n",
    "    for param in model.parameters():\n",
    "        ind_end = ind_start + np.prod(param.shape)\n",
    "        param.data.copy_(flat_params[ind_start : ind_end].view(param.shape))\n",
    "        ind_start = ind_end\n",
    "        \n",
    "        \n",
    "def get_flat_grads(model, loss):\n",
    "    \"\"\"\n",
    "    Walkaround for computing grads in case loss does not depend on some leafs\n",
    "    TODO: remove `try` later\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = []\n",
    "    #for name, param in model.named_parameters():\n",
    "    for param in model.parameters():\n",
    "        try:\n",
    "            grads.append(torch.autograd.grad(loss, param, create_graph=True)[0])\n",
    "        except RuntimeError:\n",
    "            grads.append(torch.zeros_like(param))\n",
    "            \n",
    "    return torch.cat([grad.view(-1) for grad in grads])\n",
    "        \n",
    "        \n",
    "def cg(matvec, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    Solves system Ax=b via conjugate gradients method.\n",
    "    Adapted from John Schulman's code:\n",
    "    https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py\n",
    "    Arguments:\n",
    "        matvec        --  matrix-vector product function\n",
    "        b             --  right-hand side\n",
    "        cg_iters      --  number of iterations\n",
    "        residual_tol  --  tolerance\n",
    "    \"\"\"\n",
    "    \n",
    "    x = torch.zeros(b.size())\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    \n",
    "    for i in range(cg_iters):\n",
    "        Ap = matvec(p)\n",
    "        alpha = rdotr / torch.dot(p, Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "        newrdotr = torch.dot(r, r)\n",
    "        beta = newrdotr / rdotr\n",
    "        p = r + beta * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "            \n",
    "    return x\n",
    "\n",
    "def linesearch(f, x, fullstep, expected_improve_rate, max_backtracks=10, accept_ratio=.1):\n",
    "    \"\"\"\n",
    "    Backtracking linesearch for finding optimal proposed step size.\n",
    "    Adapted from John Schulman's code:\n",
    "    https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py\n",
    "    Arguments:\n",
    "        f                      --  \n",
    "        x                      --  \n",
    "        fullstep               --  \n",
    "        expected_improve_rate  --  \n",
    "        max_backtracks         --\n",
    "        accept_ratio           --\n",
    "    \"\"\"\n",
    "    fval = f(x)\n",
    "    for stepfrac in .5**np.arange(max_backtracks):\n",
    "        xnew = x + stepfrac*fullstep\n",
    "        newfval = f(xnew)\n",
    "        actual_improve = (fval - newfval).data[0]\n",
    "        expected_improve = expected_improve_rate*stepfrac\n",
    "        ratio = actual_improve/expected_improve\n",
    "        if ratio > accept_ratio and actual_improve > 0:\n",
    "            return True, xnew\n",
    "    return False, x\n",
    "\n",
    "\n",
    "def hess_vec_full(vec, model, grads, damping):\n",
    "    grads_vec = torch.dot(grads, Variable(vec))\n",
    "    res = get_flat_grads(model, grads_vec).data\n",
    "    return res + damping * vec\n",
    "\n",
    "\n",
    "def compute_obj_full(flat_params, agent, tss, gamma, lambda_gae):\n",
    "    # TODO: rewrite in vector form later and probably without new TrajStats \n",
    "    # TODO: and probably rewrite TrajStats, e.g. append to dict, make calc_gae not a method of class\n",
    "    \n",
    "    set_flat_params(agent, flat_params)\n",
    "    \n",
    "    res = 0\n",
    "    for ts in tss:\n",
    "        cur_ts = TrajStats()\n",
    "        for s, a, r in ts.get_sar():\n",
    "            logits, value = agent.forward(np.array([s]))\n",
    "            cur_ts.append(r, F.log_softmax(logits[0], dim=-1)[a], value[0], logits[0], s, a)\n",
    "\n",
    "        old_logs_pi = ts.get_logs_pi_a()\n",
    "        advantages = cur_ts.calc_gaes(gamma, lambda_gae)\n",
    "        logs_pi = cur_ts.get_logs_pi_a()\n",
    "        res += -(torch.exp(logs_pi - old_logs_pi) * advantages.detach()).sum()\n",
    "    \n",
    "    return res / len(tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def check_vol():\n",
    "    return len(list(filter(lambda x: x.volatile, list(agent.parameters())))) == 0\n",
    "\n",
    "def get_flat_grads_without_try(model, loss):\n",
    "    \"\"\"\n",
    "    temporary, remove it later\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        grads.append(torch.autograd.grad(loss, param, create_graph=True)[0])\n",
    "            \n",
    "    return torch.cat([grad.view(-1) for grad in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_damping = 1e-3\n",
    "max_kl = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, envs, optimizer, n_timesteps=1e5, gamma=0.99, lambda_gae=0.95, entr_coef=1e-3, log_interval=1e4):\n",
    "    \"\"\"\n",
    "    Optimize networks parameters via interacting with env\n",
    "    Arguments:\n",
    "        agent           --  agent to optimize\n",
    "        envs            --  list of environments to interact with\n",
    "        n_episodes      --  number of full interaction emulations, int\n",
    "        lambda_gae      --  mixing coefficient in generalized advantage estimation\n",
    "        entr_coef       --  entropy loss multiplier, float\n",
    "        log_interval    --  number of timesteps to print debug info, int\n",
    "    \"\"\"\n",
    "\n",
    "    n_envs = len(envs)\n",
    "    w_envs = SubprocEnvs(envs)\n",
    "\n",
    "    agent.train()\n",
    "    returns = []\n",
    "    timestep = 0\n",
    "    timestep_diff = 0\n",
    "    episode = 0\n",
    "\n",
    "    while timestep < n_timesteps:\n",
    "        states = w_envs.reset()\n",
    "        tss = [TrajStats() for _ in range(n_envs)]\n",
    "\n",
    "        episode += 1\n",
    "        while w_envs.has_alive_envs():\n",
    "            logits, value = agent.forward(states)\n",
    "            actions = agent.sample_action(logits)\n",
    "\n",
    "            ind_alive = w_envs.get_indices_alive()\n",
    "            states_new, rewards, done, _ = w_envs.step(actions)      \n",
    "            \n",
    "            for i, i_alive in enumerate(ind_alive):\n",
    "                tss[i_alive].append(rewards[i], F.log_softmax(logits[i], dim=-1)[actions[i]], value[i], logits[i], states[i], actions[i])\n",
    "            states = states_new[np.logical_not(done)]\n",
    "\n",
    "            timestep_diff += len(ind_alive)\n",
    "            timestep += len(ind_alive)\n",
    "            if timestep_diff >= log_interval:\n",
    "                timestep_diff -= log_interval\n",
    "                print('{} timesteps, av. return: {:.3f}'.format((timestep // log_interval) * log_interval, \n",
    "                                                                np.mean(returns[-300:])))\n",
    "        \n",
    "        critic_loss = 0\n",
    "        for ts in tss:\n",
    "            #entropy += -(aprobs_var * torch.exp(aprobs_var)).sum()\n",
    "            #critic_loss += 0.5*advantages.pow(2).sum()\n",
    "            episode_returns = ts.calc_episode_returns(gamma)\n",
    "            critic_loss += 0.5*(ts.get_values() - episode_returns).pow(2).sum()\n",
    "            returns.append(ts.calc_return(gamma))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (critic_loss / n_envs).backward(retain_graph=True)\n",
    "        \n",
    "        actor_loss = 0\n",
    "        for ts in tss:\n",
    "            advantages = ts.calc_gaes(gamma, lambda_gae)\n",
    "            logs_pi = ts.get_logs_pi_a()\n",
    "            actor_loss += -(logs_pi * advantages.detach()).sum()  # minus added in order to ascend\n",
    "            \n",
    "        flat_grads = get_flat_grads(agent, actor_loss).data\n",
    "        if np.allclose(flat_grads.numpy(), 0):\n",
    "            print('zero gradients, passing')\n",
    "            continue\n",
    "        \n",
    "        kl = 0\n",
    "        for ts in tss:\n",
    "            logits = ts.get_logits()\n",
    "            kl += compute_kl(logits, logits.detach())\n",
    "        flat_grads_kl = get_flat_grads(agent, kl)\n",
    "\n",
    "        hess_vec = lambda vec: hess_vec_full(vec, agent, flat_grads_kl, cg_damping)\n",
    "        \n",
    "        stepdir = cg(hess_vec, -flat_grads)\n",
    "        shs = 0.5 * torch.dot(stepdir, hess_vec(stepdir))\n",
    "        lm = np.sqrt(shs / max_kl)\n",
    "        proposed_step = stepdir / lm\n",
    "        neggdotstepdir = torch.dot(-flat_grads, stepdir)\n",
    "        \n",
    "        compute_obj = lambda flat_params: compute_obj_full(flat_params, agent, tss, gamma, lambda_gae)\n",
    "            \n",
    "        params_prev = get_flat_params(agent)\n",
    "        success, params_new = linesearch(compute_obj, params_prev, proposed_step, neggdotstepdir / lm)\n",
    "        set_flat_params(agent, params_new)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    w_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 timesteps, av. return: 0.018\n",
      "10000 timesteps, av. return: 0.018\n",
      "15000 timesteps, av. return: 0.029\n",
      "20000 timesteps, av. return: 0.012\n",
      "25000 timesteps, av. return: 0.012\n",
      "30000 timesteps, av. return: 0.024\n",
      "35000 timesteps, av. return: 0.017\n",
      "40000 timesteps, av. return: 0.027\n",
      "45000 timesteps, av. return: 0.014\n",
      "50000 timesteps, av. return: 0.015\n",
      "55000 timesteps, av. return: 0.015\n",
      "60000 timesteps, av. return: 0.027\n",
      "65000 timesteps, av. return: 0.032\n",
      "70000 timesteps, av. return: 0.012\n",
      "75000 timesteps, av. return: 0.032\n",
      "80000 timesteps, av. return: 0.015\n",
      "85000 timesteps, av. return: 0.023\n",
      "90000 timesteps, av. return: 0.009\n",
      "95000 timesteps, av. return: 0.024\n",
      "100000 timesteps, av. return: 0.024\n"
     ]
    }
   ],
   "source": [
    "n_timesteps = 100000\n",
    "gamma = 0.99\n",
    "log_interval = 5000\n",
    "batch_size = 32\n",
    "\n",
    "#env = 'CartPole-v1'\n",
    "env = 'FrozenLake-v0'\n",
    "envs = [gym.make(env) for _ in range(batch_size)]\n",
    "set_seeds(envs, 417, False)\n",
    "\n",
    "agent = AgentA2C(envs[0].observation_space, envs[0].action_space)\n",
    "optimizer = optim.Adam(agent.parameters())\n",
    "\n",
    "learn(agent, envs, optimizer, n_timesteps=n_timesteps, gamma=gamma, log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 timesteps, av. return: 18.391\n",
      "10000 timesteps, av. return: 19.651\n",
      "15000 timesteps, av. return: 18.456\n",
      "20000 timesteps, av. return: 19.216\n",
      "25000 timesteps, av. return: 20.163\n",
      "30000 timesteps, av. return: 19.106\n",
      "35000 timesteps, av. return: 19.106\n",
      "40000 timesteps, av. return: 19.182\n",
      "45000 timesteps, av. return: 18.955\n",
      "50000 timesteps, av. return: 19.007\n",
      "55000 timesteps, av. return: 18.331\n",
      "60000 timesteps, av. return: 17.736\n",
      "65000 timesteps, av. return: 18.812\n",
      "70000 timesteps, av. return: 19.171\n",
      "75000 timesteps, av. return: 18.283\n",
      "80000 timesteps, av. return: 18.682\n",
      "85000 timesteps, av. return: 18.590\n",
      "90000 timesteps, av. return: 19.135\n",
      "95000 timesteps, av. return: 18.243\n",
      "100000 timesteps, av. return: 19.382\n"
     ]
    }
   ],
   "source": [
    "n_timesteps = 100000\n",
    "gamma = 0.99\n",
    "log_interval = 5000\n",
    "batch_size = 32\n",
    "\n",
    "env = 'CartPole-v1'\n",
    "#env = 'FrozenLake-v0'\n",
    "envs = [gym.make(env) for _ in range(batch_size)]\n",
    "set_seeds(envs, 417, False)\n",
    "\n",
    "agent = AgentA2C(envs[0].observation_space, envs[0].action_space)\n",
    "optimizer = optim.Adam(agent.parameters())\n",
    "\n",
    "learn(agent, envs, optimizer, n_timesteps=n_timesteps, gamma=gamma, log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
