{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import FloatTensor as T\n",
    "\n",
    "from src.agent import AgentA2C\n",
    "from src.utils import TrajStats, set_seeds, arr2var\n",
    "from src.envs_wrappers import SubprocEnvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(logits_1, logits_2):\n",
    "    \"\"\"\n",
    "    Computes KL divergence between discrete distributions\n",
    "    \"\"\"\n",
    "    probs_1 = F.softmax(logits_1, dim=-1)\n",
    "    kl_components = probs_1 * (F.log_softmax(logits_1, dim=-1) - F.log_softmax(logits_2, dim=-1))\n",
    "    return torch.mean(torch.sum(kl_components, dim=1))\n",
    "\n",
    "\n",
    "def get_flat_params(model):\n",
    "    return torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "\n",
    "def set_flat_params(model, flat_params):\n",
    "    ind_start = 0\n",
    "    for param in model.parameters():\n",
    "        ind_end = ind_start + np.prod(param.shape)\n",
    "        param.data.copy_(flat_params[ind_start : ind_end].view(param.shape))\n",
    "        ind_start = ind_end\n",
    "        \n",
    "def set_flat_grads(model, flat_grads):\n",
    "    ind_start = 0\n",
    "    for param in model.parameters():\n",
    "        ind_end = ind_start + np.prod(param.shape)\n",
    "        param.grad = flat_grads[ind_start : ind_end].view(param.shape)\n",
    "        ind_start = ind_end\n",
    "        \n",
    "def get_flat_grads(model, loss, support_next_order=False):\n",
    "    \"\"\"\n",
    "    Walkaround for computing grads in case loss does not depend on some leafs\n",
    "    TODO: remove `try` later\n",
    "    \"\"\"\n",
    "    \n",
    "    if support_next_order:\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            try:\n",
    "                grads.append(torch.autograd.grad(loss, param, create_graph=True)[0])\n",
    "            except RuntimeError:\n",
    "                grads.append(Variable(torch.zeros_like(param.data)))\n",
    "    else:\n",
    "        for p in model.parameters():\n",
    "            p.grad = None\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        grads = [p.grad if p.grad is not None else Variable(torch.zeros_like(p.data))\n",
    "                 for p in model.parameters()]\n",
    "        \n",
    "    return torch.cat([grad.view(-1) for grad in grads])\n",
    "        \n",
    "def cg(matvec, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    Solves system Ax=b via conjugate gradients method.\n",
    "    Adapted from John Schulman's code:\n",
    "    https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py\n",
    "    Arguments:\n",
    "        matvec        --  matrix-vector product function\n",
    "        b             --  right-hand side\n",
    "        cg_iters      --  number of iterations\n",
    "        residual_tol  --  tolerance\n",
    "    \"\"\"\n",
    "    \n",
    "    x = torch.zeros(b.size())\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    \n",
    "    for i in range(cg_iters):\n",
    "        Ap = matvec(p)\n",
    "        alpha = rdotr / torch.dot(p, Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "        newrdotr = torch.dot(r, r)\n",
    "        beta = newrdotr / rdotr\n",
    "        p = r + beta * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "            \n",
    "    return x\n",
    "\n",
    "def linesearch(f, x, fullstep, expected_improve_rate, max_backtracks=10, accept_ratio=.05):\n",
    "    \"\"\"\n",
    "    Backtracking linesearch for finding optimal proposed step size.\n",
    "    Adapted from John Schulman's code:\n",
    "    https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py\n",
    "    Arguments:\n",
    "        f                      --  \n",
    "        x                      --  \n",
    "        fullstep               --  \n",
    "        expected_improve_rate  --  \n",
    "        max_backtracks         --\n",
    "        accept_ratio           --\n",
    "    \"\"\"\n",
    "    fval = f(x)\n",
    "    x_best = None\n",
    "    max_ratio = -1\n",
    "    for stepfrac in .5**np.arange(max_backtracks):\n",
    "        xnew = x + stepfrac*fullstep\n",
    "        newfval = f(xnew)\n",
    "        actual_improve = (newfval - fval).data[0]\n",
    "        if actual_improve > 0:\n",
    "            expected_improve = expected_improve_rate * stepfrac\n",
    "            ratio = actual_improve / expected_improve\n",
    "            if ratio > accept_ratio and ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "                x_best = xnew\n",
    "        \n",
    "    return (True, x_best) if x_best is not None else (False, x) \n",
    "\n",
    "\n",
    "def hess_vec_full(vec, model, grads, damping):\n",
    "    grads_vec = torch.dot(grads, Variable(vec))\n",
    "    res = get_flat_grads(model, grads_vec).data\n",
    "\n",
    "    return res + damping * vec\n",
    "\n",
    "\n",
    "def compute_obj_full(flat_params, agent, tss, gamma, lambda_gae):\n",
    "    # TODO: rewrite without new TrajStats \n",
    "    # TODO: and probably rewrite TrajStats, e.g. append to dict, make calc_gae not a method of class\n",
    "    \n",
    "    set_flat_params(agent, flat_params)\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    for ts in tss:\n",
    "        cur_ts = TrajStats()\n",
    "        cur_ts.rewards = ts.rewards\n",
    "        cur_ts.states = ts.states\n",
    "        cur_ts.actions = ts.actions\n",
    "        \n",
    "        cur_ts.logits, cur_ts.values = agent.forward(cur_ts.states)\n",
    "        cur_ts.values = [v for v in cur_ts.values]\n",
    "        cur_ts.logs_pi_a = F.log_softmax(cur_ts.logits, dim=-1)[np.arange(len(cur_ts.actions)), \n",
    "                                                                np.array(cur_ts.actions)]\n",
    "        cur_ts.logs_pi_a = [l for l in cur_ts.logs_pi_a]\n",
    "        \n",
    "        advantages = cur_ts.calc_gaes(gamma, lambda_gae)\n",
    "        #advantages = cur_ts.calc_advs(gamma, n_step=1)\n",
    "        old_logs_pi = ts.get_logs_pi_a()\n",
    "        logs_pi = cur_ts.get_logs_pi_a()\n",
    "        res += (torch.exp(logs_pi - old_logs_pi) * advantages.detach()).sum()\n",
    "    \n",
    "    return res / len(tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def check_vol():\n",
    "    return len(list(filter(lambda x: x.volatile, list(agent.parameters())))) == 0\n",
    "\n",
    "def get_flat_grads_without_try(model, loss):\n",
    "    \"\"\"\n",
    "    temporary, remove it later\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        grads.append(torch.autograd.grad(loss, param, create_graph=True)[0])\n",
    "            \n",
    "    return torch.cat([grad.view(-1) for grad in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_damping = 1e-3\n",
    "max_kl = 1e-2\n",
    "global crl\n",
    "crl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, envs, update_rule, n_timesteps=1e5, gamma=0.99, lambda_gae=0.99, entr_coef=1e-3, log_interval=1e4):\n",
    "    \"\"\"\n",
    "    Optimize networks parameters via interacting with env\n",
    "    Arguments:\n",
    "        agent           --  agent to optimize\n",
    "        envs            --  list of environments to interact with\n",
    "        update_rule     --  'A2C', 'TRPO' or 'K-FAC', str\n",
    "        n_timesteps     --  number of interactions with environments, int\n",
    "        lambda_gae      --  mixing coefficient in generalized advantage estimation\n",
    "        entr_coef       --  entropy loss multiplier, float\n",
    "        log_interval    --  number of timesteps to print debug info, int\n",
    "    \"\"\"\n",
    "\n",
    "    n_envs = len(envs)\n",
    "    w_envs = SubprocEnvs(envs)\n",
    "\n",
    "    agent.train()\n",
    "    returns = []\n",
    "    timestep = 0\n",
    "    timestep_diff = 0\n",
    "    \n",
    "    if update_rule == 'A2C':\n",
    "        optimizer = optim.Adam(agent.parameters())\n",
    "    elif update_rule == 'TRPO':\n",
    "        optimizer = optim.Adam(agent.net.value_head.parameters())\n",
    "    elif update_tule == 'K-FAC':\n",
    "        raise NotImplementedError\n",
    "        optimizer = KFACOptimizer(agent.parameters())\n",
    "    else:\n",
    "        raise ValueError('Unknown update rule')\n",
    "\n",
    "    while timestep < n_timesteps:\n",
    "        states = w_envs.reset()\n",
    "        tss = [TrajStats() for _ in range(n_envs)]\n",
    "\n",
    "        while w_envs.has_alive_envs():\n",
    "            logits, value = agent.forward(states)\n",
    "            actions = agent.sample_action(logits)\n",
    "\n",
    "            ind_alive = w_envs.get_indices_alive()\n",
    "            states_new, rewards, done, _ = w_envs.step(actions)      \n",
    "            \n",
    "            for i, i_alive in enumerate(ind_alive):\n",
    "                tss[i_alive].append(rewards[i], F.log_softmax(logits[i], dim=-1)[actions[i]], value[i], logits[i], states[i], actions[i])\n",
    "            states = states_new[np.logical_not(done)]\n",
    "\n",
    "            timestep_diff += len(ind_alive)\n",
    "            timestep += len(ind_alive)\n",
    "            if timestep_diff >= log_interval:\n",
    "                timestep_diff -= log_interval\n",
    "                print('{} timesteps, av. return: {:.3f}'.format((timestep // log_interval) * log_interval, \n",
    "                                                                np.mean(returns[-300:])))\n",
    "        \n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        for ts in tss:\n",
    "            episode_returns = ts.calc_episode_returns(gamma)\n",
    "            critic_loss += 0.5*(ts.get_values() - episode_returns).pow(2).sum()\n",
    "            returns.append(ts.calc_return(gamma))\n",
    "            \n",
    "            advantages = ts.calc_gaes(gamma, lambda_gae)\n",
    "            logs_pi = ts.get_logs_pi_a()\n",
    "            actor_loss += -(logs_pi * advantages.detach()).sum()  # minus added in order to ascend\n",
    "\n",
    "        global crl\n",
    "        crl.append(critic_loss.data[0])\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if update_rule == 'A2C' or update_rule == 'K-FAC':\n",
    "            ((actor_loss + critic_loss) / n_envs).backward()\n",
    "            optimizer.step()\n",
    "        elif update_rule == 'TRPO':\n",
    "            \n",
    "            critic_flat_grads = get_flat_grads(agent, critic_loss/n_envs)\n",
    "            flat_grads = get_flat_grads(agent, actor_loss).data\n",
    "            \n",
    "            if np.allclose(flat_grads.numpy(), 0):\n",
    "                print('zero gradients, passing')\n",
    "                continue\n",
    "\n",
    "            kl = 0\n",
    "            for ts in tss:\n",
    "                logits = ts.get_logits()\n",
    "                kl += compute_kl(logits, logits.detach())\n",
    "\n",
    "            flat_grads_kl = get_flat_grads(agent, kl, support_next_order=True)\n",
    "            hess_vec = lambda vec: hess_vec_full(vec, agent, flat_grads_kl, cg_damping)\n",
    "\n",
    "            stepdir = cg(hess_vec, -flat_grads, cg_iters=10)\n",
    "            shs = 0.5 * torch.dot(stepdir, hess_vec(stepdir))\n",
    "            \n",
    "            lm = np.sqrt(shs / max_kl)\n",
    "            proposed_step = stepdir / lm\n",
    "            neggdotstepdir = torch.dot(-flat_grads, stepdir)\n",
    "\n",
    "            compute_obj = lambda flat_params: compute_obj_full(flat_params, agent, tss, gamma, lambda_gae)\n",
    "            params_prev = get_flat_params(agent)\n",
    "            success, params_new = linesearch(compute_obj, params_prev, proposed_step, neggdotstepdir / lm)\n",
    "            set_flat_params(agent, params_new)\n",
    "\n",
    "            set_flat_grads(agent, critic_flat_grads)\n",
    "            optimizer.step()\n",
    "        \n",
    "    w_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 100000\n",
    "gamma = 0.995\n",
    "log_interval = 1000\n",
    "batch_size = 16\n",
    "lambda_gae = 0.97\n",
    "\n",
    "env = 'CartPole-v1'\n",
    "#env = 'FrozenLake-v0'\n",
    "envs = [gym.make(env) for _ in range(batch_size)]\n",
    "set_seeds(envs, 0, False)\n",
    "\n",
    "agent = AgentA2C(envs[0].observation_space, envs[0].action_space)\n",
    "update_rule = 'TRPO'\n",
    "\n",
    "learn(agent, envs, update_rule, n_timesteps=n_timesteps, gamma=gamma, \n",
    "      lambda_gae=lambda_gae, log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (crl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # TEST\n",
    "            if False:\n",
    "                for p in agent.parameters():\n",
    "                    p.grad = None\n",
    "                actor_loss.backward()\n",
    "                grads = torch.cat([ \n",
    "                    p.grad.data.view(-1) \n",
    "                    for p in agent.parameters() if p.grad is not None\n",
    "                ])\n",
    "                \n",
    "                print (grads.size())\n",
    "                print (flat_grads.size())\n",
    "                print (grads)\n",
    "                print (flat_grads)\n",
    "                return -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
