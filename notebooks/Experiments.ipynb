{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from src.agent import AgentA2C\n",
    "from src.utils import TrajStats, set_seeds\n",
    "from src.envs_wrappers import SubprocEnvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl(logits_1, logits_2):\n",
    "    \"\"\"\n",
    "    Computes KL divergence between discrete distributions\n",
    "    \"\"\"\n",
    "    \n",
    "    probs_1 = F.softmax(logits_1, dim=-1)\n",
    "    kl_components = probs_1 * (F.log_softmax(logits_1, dim=-1) - F.log_softmax(logits_2, dim=-1))\n",
    "    return torch.mean(torch.sum(kl_components, dim=0))\n",
    "\n",
    "\n",
    "def get_flat_params(model):\n",
    "    return torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "\n",
    "def set_flat_params(model, flat_params):\n",
    "    ind_start = 0\n",
    "    for param in model.parameters():\n",
    "        ind_end = ind_start + np.prod(param.shape)\n",
    "        param.data.copy_(flat_params[ind_start : ind_end].view(param.shape))\n",
    "        ind_start = ind_end\n",
    "        \n",
    "        \n",
    "def get_flat_grads(model, loss):\n",
    "    \"\"\"\n",
    "    Walkaround for computing grads in case loss does not depend on some leafs\n",
    "    TODO: remove `try` later\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = []\n",
    "    #for name, param in model.named_parameters():\n",
    "    for param in model.parameters():\n",
    "        try:\n",
    "            grads.append(torch.autograd.grad(loss, param, create_graph=True)[0])\n",
    "        except RuntimeError:\n",
    "            grads.append(torch.zeros_like(param))\n",
    "            \n",
    "    return torch.cat([grad.view(-1) for grad in grads])\n",
    "        \n",
    "        \n",
    "def cg(matvec, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    Solves system Ax=b via conjugate gradients method.\n",
    "    Adapted from John Schulman's code:\n",
    "    https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py\n",
    "    Arguments:\n",
    "        matvec        --  matrix-vector product function\n",
    "        b             --  right-hand side\n",
    "        cg_iters      --  number of iterations\n",
    "        residual_tol  --  tolerance\n",
    "    \"\"\"\n",
    "    \n",
    "    x = torch.zeros(b.size())\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    \n",
    "    for i in range(cg_iters):\n",
    "        Ap = matvec(p)\n",
    "        alpha = rdotr / torch.dot(p, Ap)\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "        newrdotr = torch.dot(r, r)\n",
    "        beta = newrdotr / rdotr\n",
    "        p = r + beta * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "            \n",
    "    return x\n",
    "\n",
    "def linesearch(f, x, fullstep, expected_improve_rate, max_backtracks=10, accept_ratio=.1):\n",
    "    \"\"\"\n",
    "    Backtracking linesearch for finding optimal proposed step size.\n",
    "    Adapted from John Schulman's code:\n",
    "    https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py\n",
    "    Arguments:\n",
    "        f                      --  \n",
    "        x                      --  \n",
    "        fullstep               --  \n",
    "        expected_improve_rate  --  \n",
    "        max_backtracks         --\n",
    "        accept_ratio           --\n",
    "    \"\"\"\n",
    "    fval = f(x)\n",
    "    for stepfrac in .5**np.arange(max_backtracks):\n",
    "        xnew = x + stepfrac*fullstep\n",
    "        newfval = f(xnew)\n",
    "        actual_improve = (fval - newfval).data[0]\n",
    "        expected_improve = expected_improve_rate*stepfrac\n",
    "        ratio = actual_improve/expected_improve\n",
    "        if ratio > accept_ratio and actual_improve > 0:\n",
    "            return True, xnew\n",
    "    return False, x\n",
    "\n",
    "\n",
    "def hess_vec_full(vec, model, grads, damping):\n",
    "    grads_vec = torch.dot(grads, Variable(vec))\n",
    "    res = get_flat_grads(model, grads_vec).data\n",
    "    return res + damping * vec\n",
    "\n",
    "\n",
    "def compute_obj_full(flat_params, agent, tss, gamma, lambda_gae):\n",
    "    # TODO: rewrite in vector form later and probably without new TrajStats \n",
    "    # TODO: and probably rewrite TrajStats, e.g. append to dict, make calc_gae not a method of class\n",
    "    \n",
    "    set_flat_params(agent, flat_params)\n",
    "    \n",
    "    res = 0\n",
    "    for ts in tss:\n",
    "        cur_ts = TrajStats()\n",
    "        for s, a, r in ts.get_sar():\n",
    "            logits, value = agent.forward(s)\n",
    "            cur_ts.append(r, F.log_softmax(logits, dim=-1)[a], value, logits, s, a)\n",
    "\n",
    "        old_logs_pi = ts.get_logs_pi_a()\n",
    "        advantages = cur_ts.calc_gaes(gamma, lambda_gae)\n",
    "        logs_pi = cur_ts.get_logs_pi_a()\n",
    "        res += -(torch.exp(logs_pi - old_logs_pi) * advantages.detach()).sum()\n",
    "    \n",
    "    return res / len(tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def check_vol():\n",
    "    return len(list(filter(lambda x: x.volatile, list(agent.parameters())))) == 0\n",
    "\n",
    "def get_flat_grads_without_try(model, loss):\n",
    "    \"\"\"\n",
    "    temporary, remove it later\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = []\n",
    "    for name, param in model.named_parameters():\n",
    "        grads.append(torch.autograd.grad(loss, param, create_graph=True)[0])\n",
    "            \n",
    "    return torch.cat([grad.view(-1) for grad in grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_damping = 1e-3\n",
    "max_kl = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, envs, optimizer, n_timesteps=1e5, gamma=0.99, lambda_gae=0.95, entr_coef=1e-3, log_interval=1e4):\n",
    "    \"\"\"\n",
    "    Optimize networks parameters via interacting with env\n",
    "    Arguments:\n",
    "        agent           --  agent to optimize\n",
    "        envs            --  list of environments to interact with\n",
    "        n_episodes      --  number of full interaction emulations, int\n",
    "        lambda_gae      --  mixing coefficient in generalized advantage estimation\n",
    "        entr_coef       --  entropy loss multiplier, float\n",
    "        log_interval    --  number of timesteps to print debug info, int\n",
    "    \"\"\"\n",
    "\n",
    "    n_envs = len(envs)\n",
    "    w_envs = SubprocEnvs(envs)\n",
    "\n",
    "    agent.train()\n",
    "    returns = []\n",
    "    timestep = 0\n",
    "    timestep_diff = 0\n",
    "    episode = 0\n",
    "\n",
    "    while timestep < n_timesteps:\n",
    "        states = w_envs.reset()\n",
    "        tss = [TrajStats() for _ in range(n_envs)]\n",
    "\n",
    "        episode += 1\n",
    "        while w_envs.has_alive_envs():\n",
    "            logits, value = agent.forward(states)\n",
    "            actions = agent.sample_action(logits)\n",
    "\n",
    "            ind_alive = w_envs.get_indices_alive()\n",
    "            states_new, rewards, done, _ = w_envs.step(actions)      \n",
    "            \n",
    "            for i, i_alive in enumerate(ind_alive):\n",
    "                tss[i_alive].append(rewards[i], F.log_softmax(logits[i], dim=-1)[actions[i]], value[i], logits[i], states[i], actions[i])\n",
    "            states = states_new[np.logical_not(done)]\n",
    "\n",
    "            timestep_diff += len(ind_alive)\n",
    "            timestep += len(ind_alive)\n",
    "            if timestep_diff >= log_interval:\n",
    "                timestep_diff -= log_interval\n",
    "                print('{} timesteps, av. return: {:.3f}'.format((timestep // log_interval) * log_interval, \n",
    "                                                                np.mean(returns[-300:])))\n",
    "        \n",
    "        critic_loss = 0\n",
    "        for ts in tss:\n",
    "            #entropy += -(aprobs_var * torch.exp(aprobs_var)).sum()\n",
    "            #critic_loss += 0.5*advantages.pow(2).sum()\n",
    "            episode_returns = ts.calc_episode_returns(gamma)\n",
    "            critic_loss += 0.5*(ts.get_values() - episode_returns).pow(2).sum()\n",
    "            returns.append(ts.calc_return(gamma))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (critic_loss / n_envs).backward(retain_graph=True)\n",
    "        \n",
    "        actor_loss = 0\n",
    "        for ts in tss:\n",
    "            advantages = ts.calc_gaes(gamma, lambda_gae)\n",
    "            logs_pi = ts.get_logs_pi_a()\n",
    "            actor_loss += -(logs_pi * advantages.detach()).sum()  # minus added in order to ascend\n",
    "            \n",
    "        flat_grads = get_flat_grads(agent, actor_loss).data\n",
    "        if np.allclose(flat_grads.numpy(), 0):\n",
    "            print('zero gradients, passing')\n",
    "            continue\n",
    "        \n",
    "        kl = 0\n",
    "        for ts in tss:\n",
    "            logits = ts.get_logits()\n",
    "            kl += compute_kl(logits, logits.detach())\n",
    "        flat_grads_kl = get_flat_grads(agent, kl)\n",
    "\n",
    "        hess_vec = lambda vec: hess_vec_full(vec, agent, flat_grads_kl, cg_damping)\n",
    "        \n",
    "        stepdir = cg(hess_vec, -flat_grads)\n",
    "        shs = 0.5 * torch.dot(stepdir, hess_vec(stepdir))\n",
    "        lm = np.sqrt(shs / max_kl)\n",
    "        proposed_step = stepdir / lm\n",
    "        neggdotstepdir = torch.dot(-flat_grads, stepdir)\n",
    "        \n",
    "        compute_obj = lambda flat_params: compute_obj_full(flat_params, agent, tss, gamma, lambda_gae)\n",
    "            \n",
    "        params_prev = get_flat_params(agent)\n",
    "        success, params_new = linesearch(compute_obj, params_prev, proposed_step, neggdotstepdir / lm)\n",
    "        set_flat_params(agent, params_new)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    w_envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 timesteps, av. return: 17.976\n",
      "2000 timesteps, av. return: 18.372\n",
      "3000 timesteps, av. return: 18.475\n",
      "4000 timesteps, av. return: 18.711\n",
      "5000 timesteps, av. return: 18.823\n",
      "6000 timesteps, av. return: 19.208\n",
      "7000 timesteps, av. return: 19.023\n",
      "8000 timesteps, av. return: 19.180\n",
      "9000 timesteps, av. return: 19.124\n",
      "10000 timesteps, av. return: 19.165\n",
      "11000 timesteps, av. return: 19.025\n",
      "12000 timesteps, av. return: 18.855\n",
      "13000 timesteps, av. return: 18.262\n",
      "14000 timesteps, av. return: 18.359\n",
      "15000 timesteps, av. return: 18.099\n",
      "16000 timesteps, av. return: 18.343\n",
      "17000 timesteps, av. return: 18.277\n",
      "18000 timesteps, av. return: 18.516\n",
      "19000 timesteps, av. return: 18.955\n",
      "20000 timesteps, av. return: 18.809\n"
     ]
    }
   ],
   "source": [
    "n_timesteps = 20000\n",
    "gamma = 0.99\n",
    "log_interval = 1000\n",
    "batch_size = 16\n",
    "\n",
    "env = 'CartPole-v1'\n",
    "#env = 'FrozenLake-v0'\n",
    "envs = [gym.make(env) for _ in range(batch_size)]\n",
    "set_seeds(envs, 417, False)\n",
    "\n",
    "agent = AgentA2C(envs[0].observation_space, envs[0].action_space)\n",
    "optimizer = optim.Adam(agent.parameters())\n",
    "\n",
    "learn(agent, envs, optimizer, n_timesteps=n_timesteps, gamma=gamma, log_interval=log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
